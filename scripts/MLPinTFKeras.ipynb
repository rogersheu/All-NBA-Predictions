{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ff009f",
   "metadata": {},
   "source": [
    "##### All-Star/NBA Predictions\n",
    "\n",
    "An MLP implementation with Tensorflow and Keras.\n",
    "\n",
    "I tried to keep this code generally in line with the code I have in the scikit-learn implemented MLPmodeling.py script.\n",
    "\n",
    "First, we need to import a number of necessary packages and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "a8a9ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.activations import relu, sigmoid\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b56a8",
   "metadata": {},
   "source": [
    "I have historical player stats at this path. Make sure that data is present before using.\n",
    "\n",
    "I explain how I arrived at these 7 main features in the `README.md` file in my GitHub at https://github.com/rogersheu/NBA-ML-Predictions/blob/master/README.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "4c7b0a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All players data loaded.\n"
     ]
    }
   ],
   "source": [
    "fileName = 'C:/Users/Roger/Documents/GitHub/All-Star-Predictions/baseData/ML/all_stats_20211201.csv'\n",
    "df = pd.read_csv(fileName)\n",
    "print('All players data loaded.')\n",
    "\n",
    "X = df[['RPG','APG','SBPG','PPG','TS','WS48','Perc']]\n",
    "y = df['allLeague']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961178c",
   "metadata": {},
   "source": [
    "Because this data set is relatively imbalanced, with around 1000 all-League (either All-Star or All-NBA) player-seasons out of 7400 or so (~13.5%), I made sure to use train_test_split with stratification, to ensure each split had the same proportions of 1's and 0's and avoided small sample size biases. The likelihood of such a thing happening is relatively minimal, given that the test set still has at least 1000 members, but it's a good idea to be preempt such issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "eec6ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d73b9",
   "metadata": {},
   "source": [
    "I decided to use scikit-learn's StandardScaler, since unlike Keras' normalize function, which forces data to be between 0 and 1, the StandardScaler transform changes the data into a mean of 0 and a standard deviation of 1, which is much more preferable for a multilayer perceptron analysis. Running Keras' normalize instead of the scaler gives unfortunate results, as the model quickly converges to assigning all 1's to the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "8c586343",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a4397",
   "metadata": {},
   "source": [
    "Here, we finally set up the neural network. There exist a few other ways to do this, like the Sequential([ ... ]) method, but I like the modularity of adding layers.\n",
    "\n",
    "As mentioned in the opening block, to closely align with my scikit-learn-based code, I used a 4 node relu hidden layer followed by a 1 node sigmoid conversion. However, I also tried a pair of 2-node relu-activated hidden layers. While using such a set-up worked here adequately, using it in the scikit-learn led to odd results, with correct classifications but oddly maxed-out probabilities. Therefore, I defaulted to using the 4-node relu-activated layer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4a8a80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(4, activation='relu', name = \"hidden1\"))\n",
    "#model.add(Dense(2, activation='relu', name = \"hidden1\"))\n",
    "#model.add(Dense(2, activation='relu', name = \"hidden2\"))\n",
    "model.add(Dense(1, activation='sigmoid', name = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "60060c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3edf7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'verbose=0' to see training epochs, especially if you're suspicious of this fitting going so quietly.\n",
    "model.fit(X_train, y_train, verbose=0, epochs=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a0e10",
   "metadata": {},
   "source": [
    "We can quickly evaluate the model here, though I also included a confusion matrix and classification report below, so we will have multiple ways of evaluating our model output. Still, this is the first time we see the model used on the y_test data, and we see that our accuracy is quite good, especially if the model has enough epochs.\n",
    "\n",
    "If your accuracy is not higher than around 86% for whatever reason, you may need to run the fit again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "9ed3b58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 457us/step - loss: 0.1512 - accuracy: 0.9407\n",
      "0.15123455226421356 0.9407407641410828\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "a6fffc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test, model.predict(X_test)\n",
    "\n",
    "y_pred_bin = np.zeros([len(y_pred),1], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c2385",
   "metadata": {},
   "source": [
    "Keras, sklearn, or some other package probably contains a function to do this quickly, but I just wanted to convert model predictions into a binary array to make the confusion matrix and classification report. Python's list comprehension is a really nice way to present this conversion.\n",
    "\n",
    "Feel free to toggle the threshold if you want to lean more toward precision or recall, too, though keep in mind the impact this may have on your eventual new data predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "a6e71df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1257   20]\n",
      " [  68  140]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_bin = [1 if item > 0.5 else 0 for item in y_pred]\n",
    "        \n",
    "print(confusion_matrix(y_true, y_pred_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "6af69cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      1277\n",
      "           1       0.88      0.67      0.76       208\n",
      "\n",
      "    accuracy                           0.94      1485\n",
      "   macro avg       0.91      0.83      0.86      1485\n",
      "weighted avg       0.94      0.94      0.94      1485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2a516",
   "metadata": {},
   "source": [
    "Once we are happy with our model, we can use it on some new data.\n",
    "\n",
    "In the following block, I bring in data from the ongoing season, extracting the same columns as the historical data.\n",
    "\n",
    "After transforming the data as before and using those transformed data for the prediction, we get all-League probabilities in the variable df_2022pred, which can then be exported as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "a4633d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = pd.read_csv(\"C:/Users/Roger/Documents/GitHub/All-Star-Predictions/baseData/dailystats/2022-01-06/stats_20220106.csv\")\n",
    "X_2022 = df_2022[['RPG','APG','SBPG','PPG','TS','WS48','Perc']]\n",
    "\n",
    "df_2022pred = df_2022.copy()\n",
    "X_2022 = scaler.transform(X_2022)\n",
    "X_predicted = model.predict(X_2022)\n",
    "df_2022pred['Prob'] = [i[0] for i in X_predicted]\n",
    "df_2022pred; # Export to target CSV if so desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82348de",
   "metadata": {},
   "source": [
    "That's it! If you have made it this far, thank you for reading through my implementation of a multilayer perceptron (MLP) using TensorFlow/Keras, along with some scikit-learn tools (splitting, scaling, and output evaluation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
